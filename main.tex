\documentclass[12pt,a4paper,openright,twoside]{book}
\usepackage[utf8]{inputenc}
\usepackage{disi-thesis}
\usepackage{code-lstlistings}
\usepackage{float}
\usepackage{notes}
\usepackage{shortcuts}
\usepackage{listings}
\usepackage{acronym}
\usepackage{hyperref} % links
\usepackage{comment} % for multi-line comments
\usepackage{booktabs} % for better tables
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{svg}

\input{yaml-def}


 \lstdefinestyle{my-kotlin}{
	showstringspaces=false,
	basicstyle=\scriptsize\ttfamily,
	commentstyle={\color{olive}},
	identifierstyle=\color{black},
	ndkeywordstyle={\color{blue}},
	stringstyle={\color{magenta}},
	emphstyle={},
	keywordstyle={\color{blue}},
	breaklines=true,
	numbers=left,
	numberstyle=\tiny\color{gray},
	numbersep=10pt,
	tabsize=2,
	extendedchars=true,
	frame=trBL,
	frameround=fttt,
}

 \lstdefinelanguage{my-kotlin}{
	comment=[l]{//},
	emph={delegate, filter, first, firstOrNull, forEach, lazy, map, mapNotNull, println, return@},
	keywords={abstract, actual, as, as?, break, by, class, companion, continue, data, do, dynamic, else, enum, expect, false, final, for, fun, get, if, import, in, interface, internal, is, null, object, override, package, private, public, return, set, super, suspend, this, throw, true, try, typealias, val, var, vararg, when, where, while},
	morecomment=[s]{/*}{*/},
	morestring=[b]",
	morestring=[s]{"""*}{*"""},
	ndkeywords={@Deprecated, @Serializable, @JvmField, @JvmName, @JvmOverloads, @JvmStatic, @JvmSynthetic, Array, Byte, Double, Float, Int, Integer, Iterable, Long, Runnable, Short, String, Unit, it},
	sensitive=true
}

\showboxdepth=5
\showboxbreadth=5

\school{\unibo}
\programme{Corso di Laurea Magistrale in Ingegneria e Scienze Informatiche}
\title{Design and Implementation of a Prototype Open Benchmarking Platform for Collective Adaptive Systems}
\author{Paolo Penazzi}
\date{\today}
\subject{Laboratorio di Sistemi Software}
\supervisor{Prof. Danilo Pianini}
\cosupervisor{Prof. Lukas Esterle}
\session{III}
\academicyear{2022-2023}

% Definition of acronyms
\acrodef{CAS}{Collective Adaptive Systems}
\acrodef{vm}[VM]{Virtual Machine}
\acrodef{SUT}{System Under Test}
\acrodef{CLI}{Command Line Interface}


\mainlinespacing{1.241} % line spacing in main matter, comment to default (1)

\begin{document}

\frontmatter\frontispiece

\begin{abstract}
  In every domain of scientific research, the comparison between innovative solutions and the state of the art is crucial.
  This practice enables the evaluation of whether the system under examination outperforms the established reference, either comprehensively or in specific aspects.
  In various fields of computer science, tools have been developed to benchmark new and existing solutions.
  On the other hand, in the domain of collective adaptive systems, a conspicuous gap exists in software designed to facilitate such comparisons.

  The primary objective of this thesis is to create a prototype for a benchmarking platform focused on Collective Adaptive Systems (CAS). 
  By making use of existing simulators available in the market, the aim is to establish a comprehensive framework for testing, validating, and comparing these dynamic systems. 
  The presented platform is designed to allow users to define benchmarks, execute them, and extract results of interest - all while preserving flexibility and extensibility. 
  This inherent adaptability allows for the incorporation of additional simulators into the testbed.

  An experiment has been executed to validate the framework's anticipated functionalities and understand its strengths and weaknesses. 
  This analysis serves the purpose of identifying areas for future improvement within the tool.
\end{abstract}
  
\begin{acknowledgements}
  
\end{acknowledgements}

%----------------------------------------------------------------------------------------
\tableofcontents
\listoffigures
\lstlistoflistings
%----------------------------------------------------------------------------------------

\mainmatter

%----------------------------------------------------------------------------------------
\chapter{Introduction}
\label{chap:introduction}
%----------------------------------------------------------------------------------------
\paragraph{Background and Motivation}

In scientific research, new solutions are developed in order to improve what is defined as the state of art. 
This implies that, at some point, two different solutions must be compared. 
To define which one performs better, either overall or in specific aspects, it is necessary to employ a clear and objective metric. 
Additionally, it is essential to use a widely adopted, standard test protocol that executes two different algorithms on the same problem, under the same conditions, with the possibility of reproducing experiments. 
The lack of such a protocol raises concerns regarding the reproducibility of scientific publications, as happened in some fields of Computer Science \cite{DBLP:journals/cacm/CollbergP16, DBLP:conf/aaai/GundersenK18}.
Therefore, the creation of a benchmarking and testing framework becomes necessary to set a standard for the comparison of different solutions.
In some application domains, such as security \cite{DBLP:conf/bdet/Es-SamaaliOBMK21}, IoT \cite{DBLP:conf/IEEEares/RuckGWLN23}, and many more \cite{DBLP:journals/ral/CollinsRYSJP24, DBLP:journals/corr/abs-2401-01275}, tools have been developed for benchmarking. 
In other fields, like Autonomic, Organic Computing, and \ac*{CAS}, such frameworks have not been created yet \cite{DBLP:conf/icac/BrownHHLLSY04, DBLP:conf/autonomics/EtcheversCV09}.
This absence led us to focus on \ac*{CAS} \cite{DBLP:journals/sttt/NicolaJW20}. 
These systems are characterized by their ability to adapt to the environment in which they are immersed and to interact with it \cite{DBLP:conf/birthday/BucchiaroneM19}.
There is no central entity, either internal or external, that coordinates the devices in the network. Instead, each node collaborates with its neighbor to achieve a goal.
The properties and behavior of these systems make them particularly challenging to test and evaluate their performance \cite{DBLP:conf/srds/AlmeidaMV10}.

\paragraph{Objectives}
This thesis aims to partially fill this gap by developing a prototype of a benchmarking platform \cite{DBLP:conf/cisis/VilenicaL12, DBLP:conf/atal/ZhangZWBR20}.
Specifically, the objective is to create a standard way for the scientific community to compare different solutions to typical problems in the \ac*{CAS} domain.
This platform is designed to allow the user to define a benchmark, execute it, and compare the result to state-of-the-art solutions.
Given the great number of simulators already available, the framework must be flexible and extensible.
In this way, it is possible to add new simulators without compromising the functionality of the already supported ones \cite{DBLP:conf/mascots/Dujmovic99}.
Furthermore, users might want to evaluate different aspects of the \ac*{CAS}. Therefore, the metrics for evaluating them must be generic and customizable.

%----------------------------------------------------------------------------------------
\chapter{Background}
%----------------------------------------------------------------------------------------

\section{Collective Adaptive Systems}

\ac{CAS} are a complex type of distributed network which are composed of many heterogeneous entities, each with its own capabilities and goals. 
These systems are characterized by the ability to adapt their behavior to dynamically changing open-ended environments and by the pursuit of a collective goal. 
The latter is achieved through the collaboration of the systemsâ€™ entities, without a specific external or internal central control \cite{DBLP:series/lncs/HolzlRW08, DBLP:journals/corr/abs-1108-5643}. 
In fact, CAS often adopts cooperative operating strategies to run distributed decision-making mechanisms \cite{DBLP:journals/tomacs/Aldini18}.

Nowadays, many systems are adaptive and collective. Examples include drone swarms tasked with monitoring an area,
wearable devices to manage crowd congestion during a public event, cars on streets connected to handle traffic \cite{DBLP:journals/sttt/NicolaJW20}.

\Cref{fig:cas-examples} is provided by \footnote{https://www.unibo.it/it/studiare/dottorati-master-specializzazioni-e-altra-formazione/insegnamenti/insegnamento/2023/483706}.

\begin{figure*}[h]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/swarm2.jpeg}
    \caption{Drone swarm.}
  \end{subfigure}

  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/crowd.png}
    \caption{Crowd management.}
    \label{fig:cas-examples}
  \end{subfigure}
  \caption{Some examples of CAS.}
\end{figure*}

\paragraph*{CAS programming}
The spread of these systems has led to a shift in computation, which is now divided and distributed across various devices in the network, introducing an additional level of complexity in programming these systems.
Developers must consider issues such as communication between devices, concurrency, or failures. 
Furthermore, as these systems grow in complexity, it becomes challenging to create solutions that are extensible, modular, and easily testable \cite{DBLP:conf/ecoop/CasadeiV16}.
Several approaches have been identified over the years for programming CAS, or more generally, adaptive distributed systems.
Agent-based models \cite{MacalN10} associate each device with the behavior of an agent \cite{RussellN95} Agent, which has sensors and actuators to interact with the environment and the ability to communicate with other agents. 
This is the approach used in the NetLogo simulator.
The SCEL language \cite{NicolaLPT14}, the first to use the paradigm defined as attribute-based programming, establishes a formal approach to the interaction between devices. 
This paradigm defines the system as a series of devices, each with a set of attributes representing the properties of the component.
Aggregate programming is a new approach to developing complex distributed systems that abstract from individual devices, focusing on programming the collective.
Through a layer that handles and hides some problematic aspects of these networks, such as communication between devices and details of individual entities,
it is possible to simplify the design and maintenance of these systems \cite{DBLP:journals/computer/BealPV15, DBLP:conf/sfm/BealV16}.
Aggregate programming is based on the concept of a computational field, which is a global map associating each device in the network with its local value,
and on that of field calculus, a minimal core that provides basic constructs for working with fields \cite{DBLP:journals/corr/ViroliADPB16}.
Other notable approaches exist, including SOTA \cite{AbeywickramaBMZ20} and TOTA \cite{MameiVZ04}. 

\section{Testing and Simulation}

In every field of engineering, testing is a fundamental part of the development process. 
It refers to the process carried out to verify and validate a system, according to its requirements \cite{Spillner2011}.
Testing is important to evaluate the behavior of newly developed algorithms against state-of-the-art solutions.
This allows us to understand whether a newly developed solution is better than an existing one in a certain scenario. \\

Adaptive systems testing introduces a series of challenges and difficulties, many of which stem from the intrinsic nature of these systems.
Given their complexity, the use of a single simulator is not sufficient to test all their features.
In fact, it is often necessary to employ multiple simulators and combine the results obtained to understand the system's behavior.
This technique is termed co-simulation and introduces various issues, such as communication delays, approximations, and difficulties in synchronizing simulators \cite{DBLP:journals/simpra/ThuleLGML19}.
Since these systems are adaptive and react to mutations in their environment, it is natural to want to support the injection of changes \cite{DBLP:conf/icac/BrownHHLLSY04}.
Therefore, a complete testbed must be able to command different existing tools, support various execution environments, and allow the user to test the system in its entirety. \\

In Computer Science, the simulation is the process of executing software in a controlled environment to evaluate its behavior.
Simulations can be used to test the correctness of a program, evaluate its performance, or understand its behavior in a specific scenario.
The key point of a simulation is to execute the software under controlled and repeatable conditions to compare different executions  \cite{DBLP:journals/cacm/CollbergP16}.
This cannot be done without a simulator, which provides the user with all the tools needed to run the simulation  \cite{argun2021simulation, bagrodia1998parsec}. \\
The importance of simulators becomes clear when testing \ac{CAS}.
Creating a real environment to test a program, such as a network of 100 drones or a crowd of 1000 people, is not feasible.
Simulators allow us to deploy a virtual environment where we can run the program and evaluate its behavior. \\

Within the domain of \ac{CAS}, numerous simulators provide an environment for testing these systems, either in some aspects or in their entirety.
This is the case for ns-3 \footnote{https://www.nsnam.org/}, a discrete-event network simulator, the Repast suite \cite{North2013}, a family of agent-base modeling platforms, and OMNeT++ \footnote{https://omnetpp.org/}, a modular, component-base C++ simulation library.
Given their central role in this study, the subsequent paragraphs will extensively examine other simulators, specifically Alchemist and NetLogo.

\paragraph*{Alchemist}

Alchemist \cite{Pianini_2013} is an open-source tool for simulating complex distributed systems. It is termed a meta-simulator because it is based on generic abstractions.
The meta-model of Alchemist, as \Cref{fig:alchemist-meta-model, fig:alchemist-reaction} show, is inspired by biochemistry and consists of various entities \footnote{https://alchemistsimulator.github.io/}:
\begin{itemize}
  \item \textbf{Molecule} The name of a data item.
  \item \textbf{Concentration} The value associated with a molecule.
  \item \textbf{Node} A container of molecules and concentrations. Disposed inside the environment.
  \item \textbf{Environment} The abstraction for the space. It contains nodes and can tell the position of a node in the space and the distance between two nodes.
  \item \textbf{Linking Rule} A rule that defines the relation between nodes.
  \item \textbf{Reaction} Events fired according to a time distribution and set of conditions.
  \item \textbf{Condition} A function that takes the current environment as input and outputs a boolean and a number. The output influences the execution of the corresponding reaction.
  \item \textbf{Action} Models a change in the environment.
\end{itemize}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{figures/alchemist-model.png}
  \caption{Alchemist meta-model.}
  \label{fig:alchemist-meta-model}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{figures/alchemist-reaction.png}
  \caption{Alchemist reaction.}
  \label{fig:alchemist-reaction}
\end{figure}

The key to the Alchemist's extensibility is the very generic interpretation of molecules and concentrations. An incarnation maps this generic chemical abstraction to a specific use case.
Alchemist supports four incarnations:
SAPERE \cite{DBLP:conf/saso/CastelliMRZ11}, the first supported incarnation, based on the concept of Live Semantic Annotation (LSA),
ScaFi \cite{DBLP:journals/softx/CasadeiVAP22}, which is a Scala-based library and framework for Aggregate Programming,
Protelis \cite{DBLP:conf/sac/PianiniVB15}, a programming language for aggregate computing, and
Biochemistry incarnation.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{figures/alchemist.png}
  \caption{A grid of nodes in Alchemist.}
\end{figure}

\paragraph*{NetLogo}

NetLogo\footnote{https://ccl.northwestern.edu/netlogo/} is a programmable modeling environment for simulating natural and social phenomena. It was created at the Center
for Connected Learning and Computer-Based Modeling (CCL) at Northwestern University, directed by Uri Wilensky.

NetLogo is particularly well suited for modeling complex systems developing over time.
Users can program the behavior of thousands of independent agents to see how the system-level behavior emerges from the interactions of the agents.

It also comes with the Models Library, a large collection of pre-written simulations that can be used and modified.
These simulations can be explored to observe their behavior under various conditions.

An example of a NetLogo simulation is depicted in \Cref{fig:netlogo-simulation}.
It uses the Fire model which simulates the spread of a fire through a forest. 
It shows that the fire's chance of reaching the right edge of the forest depends critically on the density of trees.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/NetLogo-interface.png}
  \caption{NetLogo interface.}
  \label{fig:netlogo-simulation}
\end{figure}

%----------------------------------------------------------------------------------------
\chapter{Design}
%----------------------------------------------------------------------------------------

\section{Domain Analysis}

A domain-driven approach was employed in the development of this work.
To better understand the problem domain and to avoid confusion, a ubiquitous language was defined.
These concepts were then utilized in the framework development and can be found in the work.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|p{0.8\textwidth}|}
    \toprule
    \textbf{Term} & \textbf{Meaning}                                                                                                                                                                                    \\
    \midrule
    Testing       & The overall process carried out to verify and validate a system, according to requirements, to promote the desired internal and external quality and to mitigate risks in development and products. \\ \hline
    Testbed       & A platform for rigorous, transparent and replicable environment for experimentation and testing                                                                                                     \\ \hline
    Solution      & A set of algorithms leading to achieving goals and overcoming the problem posted                                                                                                                    \\ \hline
    Scenario      & Contains all the information about the test execution: the simulation platform, the metrics, the input parameters                                                                                   \\ \hline
    Simulator     & A software that allows the user to see how its program would behave in a real environment                                                                                                           \\ \hline
  \end{tabular}
  \caption{Domain Ubiquitous Language}
\end{table}

User Stories were also defined to clarify the users' needs and thus what features the framework should support.

\begin{table}[H]
  \centering
  \begin{tabular}{|p{0.8\textwidth}|}
    \toprule
    \textbf{User Story}                                                                                   \\
    \midrule
    \textit{...As a user, I want to be able to create a benchmark.}                                       \\ \hline
    \textit{...As a user, I want to be able to use different simulators.}                                 \\ \hline
    \textit{...As a user, I want to be able to define and execute different scenarios.}                   \\ \hline
    \textit{...As a user, I want to be able to define a solution.}                                        \\ \hline
    \textit{...As a user, I want to be able to define how the output of the benchmark will be processed.} \\ \hline
    \textit{...As a user, I want to be able to compare my solution to others.}                       \\ \hline
  \end{tabular}
  \caption{Domain Ubiquitous Language}
\end{table}

It is worth noting that the expected users of the framework are researchers and developers, i.e., people with a strong technical background and knowledge of the domain.

\section{Requirements}

\subsection*{User Requirements}
User requirements express the needs of the user and identify which actions the user should be able to perform.
The following requirements are extracted from the previous domain analysis:
\begin{itemize}
  \item It should be possible to define a \emph{benchmark}.
  \item It should be possible to define a \emph{scenario}.
  \item It should be possible to apply a \emph{solution} to an existing scenario.
  \item It should be possible to download and use different \emph{simulators}.
  \item It should be possible to execute a benchmark.
  \item It should be possible to define which \emph{metric} to extract from the benchmark's output.
  \item It should be possible to compare the results of different solutions.
  \item It should be possible to extend the framework to support new simulators.
\end{itemize}

\subsection*{Functional Requirements}
Functional requirements define the features and the functions of the framework.
These derive from the user requirements.

\begin{itemize}
  \item The framework should allow the user to define a benchmark.
  \item The framework should allow the user to define a scenario.
  \item The framework should allow the user to run a scenario with any solution.
  \item The framework should allow the user to use different simulators, providing a way to download them.
  \item The framework should allow the user to execute a benchmark.
  \item The framework should allow the user to define which metric to extract from the benchmark's output.
  \item The framework should allow the user to compare the results of different solutions.
  \item The framework should allow the user to add support for new simulators.
\end{itemize}

\subsection*{Non-Functional Requirements}
Non-functional requirements define the quality attributes of the framework.

\begin{itemize}
  \item The framework should facilitate the user in testing collective adaptive systems.
  \item The framework should not limit the user in any way, to the extent that specific simulators permit.
  \item The framework must provide an easy and clean way to define a benchmark and all its components.
  \item The framework must provide an easy and clean API to add support for new simulators.
\end{itemize}

\section{Architecture}

The testbed is a framework that sits between the user and the various simulators.
The user specifies which benchmark to run.
The execution of the benchmark is then handled by the testbed, which takes care of running the various scenarios in the respective simulators and collecting the results.

\Cref{fig:high-level-architecture} depicts the architecture of the testbed at the highest level.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{figures/architecture-high-level.pdf}
  \caption{Abstract architecture of the testbed.}
  \label{fig:high-level-architecture}
\end{figure}

Diving deeper into the architecture, we can see that the testbed consists of different components, each with a specific role.
\Cref{fig:detailed-architecture} is a more detailed image of the system's architecture.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{figures/testbed-architecture.pdf}
  \caption{Architecture of the system.}
  \label{fig:detailed-architecture}
\end{figure}

The main component of the Testbed is the controller, which is the entry point of the framework and handles the entire benchmark execution.
The Parser is the component responsible for translating user-written specifications in YAML into a data structure that represents the benchmark model.
If there are manipulations to be made on a configuration file, they will be performed by a Parser component, specific to each simulator, before the actual parsing.
The Executor is responsible for starting the simulator and generating the correct command to invoke the simulator.
For each started simulator, the corresponding Listener is then launched. 
The latter reads the simulator's output, cleans the file from unnecessary elements, and saves it in a data structure.
Once the benchmark execution is complete, a post-processing function is applied to the benchmark output to obtain a result of interest, and it is displayed to the user by the View.

To better understand the testbed's functioning, it is useful to analyze the execution of a benchmark, which is depicted in \Cref{fig:benchmark-sequence-diagram}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{figures/execution-sequence-diagram.png}
  \caption{Benchmark execution sequence diagram.}
  \label{fig:benchmark-sequence-diagram}
\end{figure}

The configuration file is given as input to the controller, which performs some checks on the file's integrity.
Once passed, the controller hands the configuration file to the parser, which, after modifying the file (if necessary), returns to the controller a data structure called BenchmarkModel.
The model contains all the information about the benchmark to be executed.
At this point, each scenario defined by the user must be launched.
The execution is carried out in the order specified by the user.
For each scenario, a command to start the simulator is generated and launched.
The framework then waits for the simulation to finish.
Once it is done, the framework reads the results returned in output by the simulator and saves them in a data structure.
When the data from all scenarios has been collected, a user-defined transformation is applied to extract results of interest.
The latter are finally displayed to the user, either through the console or in a GUI.

\section{Benchmark Configuration}

The design of the input file system is a crucial aspect of the framework.
It should enable the user to define a benchmark simply and intuitively, without limiting the user in any way.
It also needs to be flexible enough to allow the addition of new simulators without breaking the existing structure.

The input file is composed of two main sections, namely \emph{Strategy} and \emph{Simulators}.

\paragraph*{Strategy}
The strategy section provides generic information about the testbed configuration, rather than simulator-specific instructions.
Currently, it only includes the execution order of the scenarios, a mandatory parameter that defines the sequence in which the scenarios will be executed.
Additional strategy parameters could be added in the future to support other features, such as multi-threaded execution.

\Cref{lst:benchmark-configuration-strategy} shows a possibile definition of the \emph{Strategy} section in a benchmark configuration file.

\begin{lstlisting}[language=yaml, label={lst:benchmark-configuration-strategy} ,caption={Benchmark configuration file structure: strategy section}]
  strategy:
    executionOrder:
      - Alchemist-sapere-tutorial
      - NetLogo-tutorial
      - Alchemist-protelis-tutorial
\end{lstlisting}

\paragraph*{Simulators}
The simulators section contains the configuration of the simulators used in the benchmark.
Each simulator has a name, a path, and a list of scenarios.
The name is mandatory and must be written exactly as it appears in the testbed documentation.
The path is optional and is used to specify the path of the executable simulator.
If not specified, the framework will assume that the simulator is in the same directory as the testbed.

\paragraph*{Scenario}
The scenario configuration is more complex, as it depends on which simulator is used to run the scenario.
This section contains:
\begin{itemize}
  \item \textbf{name} the name of the scenario. This is mandatory and should match the name in the execution order list.
  \item \textbf{description} a brief explanation of the scenario. This is optional.
  \item \textbf{input} a list of all the input files needed to run the scenario. This parameter is optional to take into account a scenario that does not require any input file.
  \item \textbf{postProcessing} the script that will be used to process the scenario output. This parameter is optional.
  \item \textbf{repetitions} the number of times that the scenario should be run. This parameter is optional and defaults to 1.
  \item \textbf{duration} the duration of the simulation. This parameter can be used to overwrite the value present in the simulator-specific configuration file, if the simulator supports it. This parameter is optional.
\end{itemize}

\Cref{lst:benchmark-configuration-sim1} shows a possible definition of the \emph{Simulators} section in a benchmark configuration file using NetLogo as a simulator.

\begin{lstlisting}[language=yaml, label={lst:benchmark-configuration-sim1} ,caption={Benchmark configuration file structure. NetLogo simulator section}]
simulators:
  - name: NETLOGO
    simulatorPath: "./NetLogo 6.4.0/"
    scenarios:
      - name: NetLogo-tutorial
        description: A tutorial to NetLogo
        input:
          - "../src/main/resources/netlogo/netlogo-tutorial.xml"
          - "./models/IABM Textbook/chapter 4/Wolf Sheep Simple 5.nlogo"
        repetitions: 3
        postProcessing: "./processing/netlogo-tutorial.py"
\end{lstlisting}

\Cref{lst:benchmark-configuration-sim2} depicts the configuration of two scenarios runned by the Alchemist simulator.

\begin{lstlisting}[language=yaml, label={lst:benchmark-configuration-sim2} ,caption={Benchmark configuration file structure. Alchemist simulator section}]
  simulators:
    - name: Alchemist
      simulatorPath: "./"
      scenarios:
        - name: Alchemist-protelis-tutorial
          description: A tutorial to Alchemist and Protelis incarnation
          input: ["src/main/resources/alchemist/protelis-tutorial.yml"]
          repetitions: 1
          duration: 10
        - name: Alchemist-sapere-tutorial
          description: A tutorial to Alchemist and Sapere incarnation
          input: ["src/main/resources/alchemist/sapere-tutorial.yml"]
          repetitions: 1
          duration: 100
  \end{lstlisting}

\section{Benchmark Results}

A critical part of the framework design is related to what is presented to the user at the end of the benchmark execution.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/output-processing.pdf}
  \caption{Simulator output processing}
  \label{fig:output}
\end{figure}

As \Cref{fig:output} shows, each simulator has its own method of providing simulation results. Certain simulators return a CSV file, others a text file, and some even utilize snapshots. 
Managing all these diverse cases within the framework is not feasible, which is why two methods are provided to obtain the desired results: through an external scripting file or by implementing a listener and a processing function.

In the first case, an external scripting file is used to process the simulator's output. 
Once the simulation is complete, the controller invokes the script specified in the Scenario configuration. 
Upon execution completion, the framework reads the results from a predefined JSON file, namely \textit{result.json}. 
This file must contain data in the form of \emph{ScenarioResult}, which will be defined below.

In the second case, the Listener interface is employed. It takes the file outputted by the simulator and transforms it into an internal framework data structure, the \emph{BenchmarkOutput}. 
A post-processing function must then be implemented, taking the \emph{BenchmarkOutput} as input and returning a \emph{BenchmarkResult}. 
This solution split the process into two phases, the read of the output simulator and the transformation of the data in a significant result.
This allows to define a method to read the output of a simulator and to use it in combination with various processing functions to obtain results of different natures.

We define \emph{output} concepts:
\begin{itemize}
  \item \textbf{Scenario Output} the output of a single scenario. It is a map that associates each metric with its value.
  \item \textbf{Benchmark Output} the output of the entire benchmark. It is a map that associates each scenario with its \emph{Scenario Output}.
\end{itemize}

These concepts represent the data returned at the end of the benchmark execution as generated by the simulator.
This data must be processed to extract useful information for the user.

We define \emph{result} concepts, which represent the data the user desires, obtained by processing the benchmark's output.
\begin{itemize}
  \item \textbf{Scenario Result} the result of a single scenario. It contains a description of the result and its value.
  \item \textbf{Benchmark Result} the result of the entire benchmark. It is a list of \emph{Scenario Result}.
\end{itemize}

\section{Extension}

One of the main goals of this work is to create a flexible system that can be extended to support different simulators.
The architecture was designed considering this objective.
Each component of the system has a general behavior, independent of the simulator but incomplete.
This will be then integrated with the specific behavior related to the simulator defined in a subclass.

Users interested in adding support for a new simulator must do the following steps:
\begin{itemize}
  \item Implement the \emph{Executor} interface.
  \item Implement the \emph{Listener} interface. Optional.
  \item If some manipulations on the input file are needed, implement the \emph{ConfigFileHandler} interface.
  \item Extend the \emph{SupportedSimulator} enum by adding the new simulator.
  \item Update the \emph{Controller} to take into account the new simulator.
\end{itemize}

\section{Simulators}

Since the framework relies on external simulators to run a benchmark, the users must have these simulators installed on their machines. 
Various solutions were evaluated to address this issue, each with its pros and cons.
\begin{itemize}
  \item Include all supported simulators in the application: while this solution ensures that the simulators are present, it has drawbacks. 
  Forcing the user to download all supported simulators may be impractical, especially if they only need a few of them or already have them installed. 
  Additionally, this would significantly increase the application size.
  \item Let the user download the simulators: this is the simplest solution, as it shifts the responsibility of downloading the required simulators to the user.
  However, it degrades the user experience as users cannot immediately use the framework after downloading it.
  \item Hybrid solution: simulators are not included in the framework. However, if the testbed does not detect them during benchmark execution, it automatically downloads them. 
  This solution is more complex but has proved to be the most effective after the analysis.
\end{itemize}

%----------------------------------------------------------------------------------------
\chapter{Implementation}
%----------------------------------------------------------------------------------------

In this chapter, we will dive deeper into the implementation of the framework.

\section{Framework}

\paragraph*{Benchmark Model}
The benchmark model is the data structure that represents the benchmark to be executed.
It matches the structure of the input file to allow easy parsing, as shown in \Cref{fig:benchmark-model}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{figures/benchmark-model.png}
  \caption{Benchmark Model}
  \label{fig:benchmark-model}
\end{figure}

Each concept of the model is implemented as a data class in Kotlin, which is a class that only contains data and does not have any functionality.
The \emph{Serializable} annotation is used to allow the model to be serialized and deserialized from YAML.
This annotation is provided by the \emph{"org.jetbrains.kotlinx:kotlinx-serialization-json"} library.

\begin{lstlisting}[style=my-kotlin, language=my-kotlin, caption={Benchmark model.}]
@Serializable
data class Benchmark(
    val strategy: Strategy,
    val simulators: List<Simulator>
)
@Serializable
data class Strategy(
    val executionOrder: List<String> = emptyList(),
)
@Serializable
data class Simulator(
    val name: SupportedSimulator,
    val simulatorPath: String = "",
    val scenarios: List<Scenario>,
)
@Serializable
data class Scenario(
    val name: String,
    val description: String = "",
    val input: List<String> = listOf(),
    val repetitions: Int = 1,
    val duration: Int = 0,
)
\end{lstlisting}

To account for the possibility of adding new simulators, the \emph{SupportedSimulator} enum was created.
This enum contains all the simulators supported by the framework. When a new simulator is added, it must be added to this enum.
This forces the user to extend each component of the system to support the new simulator.

\begin{lstlisting}[style=my-kotlin, language=my-kotlin, caption={SupportedSimulator enum}]
  enum class SupportedSimulator {
    ALCHEMIST,
    NETLOGO,
}
\end{lstlisting}

\paragraph*{Parser}
The parser is the component responsible for translating the user-written specifications in YAML into a data structure that represents the benchmark model.
The library \emph{"com.charleskorn.kaml:kaml"} is used to parse the input file.
This allows an easy and clean way to parse the input file as shown in the following code.

\begin{lstlisting}[style=my-kotlin, language=my-kotlin, caption={Parsing of the input file}]
  val inputFile = File(path)
  val benchmark = Yaml.default.decodeFromString(Benchmark.serializer(), inputFile.readText())
\end{lstlisting}

In certain scenarios, the configuration file for a scenario must be modified before being returned to the controller.
This is the case for the \emph{duration} parameter, which can be specified in the benchmark specification file.
This parameter may change the duration of the execution of a scenario, overriding the simulator's input file. 
To achieve this, a \emph{ConfigFileHandler} interface was created, which is implemented by each simulator-specific component.
This interface contains a single method, \emph{handle}, which takes as input the scenario configuration.

\begin{lstlisting}[style=my-kotlin, language=my-kotlin, caption={ConfigFileHandler interface}]
  interface ConfigFileHandler {
    fun editConfigurationFile(scenario: Scenario)
}
\end{lstlisting}

For example, the Alchemist implementation of the \emph{ConfigFileHandler} interface reads the \emph{duration} parameter from the scenario configuration and injects it to the Alchemist input file, overriding it.

\paragraph*{Listener}
The listener is implemented as an interface with a \emph{read} method, which takes the path to the CSV file as input and returns a ScenarioOutput. 
Saving simulation results in a CSV file is a standard and widely used approach, which is why this is the preferred method to read the simulator's output. 
In case it is not available as an option, the listener interface can be extended to implement a custom read function. 
The logic for reading from a CSV file is implemented directly in the Listener interface, while the \emph{clearCSV} method is not implemented and must be handled by the specific simulator's listener. 
In general, the \emph{clearCSV} function should clean the file from all unnecessary information, leaving the CSV file only with headers and values; otherwise, the reading will not be performed.

\begin{lstlisting}[style=my-kotlin, language=my-kotlin, caption={Listener interface.}]
  interface Listener {
    fun read(path: String = ""): ScenarioOutput
    fun clearCSV(path: String)
  }
\end{lstlisting}

The Alchemist implementation for the \emph{ClearCSV} method is:

\begin{lstlisting}[style=my-kotlin, language=my-kotlin, caption={CSV file cleaning in Alchemist}]
override fun clearCSV(path: String) {
    val lines = Files.readAllLines(Paths.get(path), StandardCharsets.UTF_8)
    val regexPatterns = listOf(
        Regex("#.*#"),
        Regex("#$"),
        Regex("# $"),
        Regex("# T.*"),
    )
    val dataLines = lines.filter { line ->
        !regexPatterns.any { pattern -> pattern.matches(line) }
    }
    val finalLines = dataLines.map { line ->
        val modifiedLine = line.replace(Regex("# "), "")
        modifiedLine
    }
    Files.write(Paths.get(path), finalLines, StandardCharsets.UTF_8)
}
\end{lstlisting}

\paragraph*{Output and Result}

The concepts of \emph{Scenario Output}, \emph{Benchmark Output} and \emph{Benchmark Result} are implemented as \emph{type alias} in Kotlin, which is a way to define a new type without creating a new class.
The concept of \emph{Scenario Result} is implemented as a data class.

\begin{lstlisting}[style=my-kotlin, language=my-kotlin, caption={Benchmark output and result model}]
  typealias ScenarioOutput = Map<String, List<Any>>
  typealias BenchmarkOutput = Map<String, ScenarioOutput>
  typealias BenchmarkResult = List<ScenarioResult>

  data class ScenarioResult(
    val description: String,
    val value: List<Any>,
    val visualisationType: VisualisationType,
)
\end{lstlisting}

Both the \textit{ScenarioOutput} and the \textit{ScenarioResult} use a list of \emph{Any} to represent the value.
This is done to ensure compatibility with any simulator that might be added in the future.

\paragraph*{Controller}

After parsing the benchmark configuration file the \emph{Controller} begins to execute every Scenario in the order specified by the user.
It creates a map containing all the scenarios and their respective simulators, and then iterates over the map, executing each scenario.

\begin{lstlisting}[style=my-kotlin, language=my-kotlin, caption={Controller implementation}]
  val scenarioNameOrder: List<String> = benchmark.strategy.executionOrder
  val scenarioMap: Map<String, Triple<Simulator, Scenario, Int>> = benchmark.simulators
      .flatMap { simulator ->
          simulator.scenarios.map { scenario ->
              scenario.name to Triple(simulator, scenario, scenario.repetitions)
          }
      }.toMap()
  scenarioNameOrder.forEach { scenarioName ->
      val (simulator, scenario, repetitions) = scenarioMap.getOrElse(scenarioName) {
          throw IllegalArgumentException("Scenario $scenarioName not found")
      }
      for (i in 1..repetitions) {
          runBlocking {
              println("[TESTBED] Running scenario $scenarioName in ${simulator.name} simulator. Run number $i")
              createExecutor(simulator.name, simulator.simulatorPath, scenario)
              val reader = createReader(simulator.name)
              val runName = "$scenarioName-$i"
              val metric = reader.read(simulator.simulatorPath + "export.csv")
              benchmarkOutput = benchmarkOutput + mapOf(runName to metric)
          }
      }
  }
\end{lstlisting}

It is important to note that each scenario execution is strictly sequential. A scenario is launched, the controller waits for the execution to finish, creates the Listener to read the results and only then moves on to the next scenario.
All the results are saved in a data structure that will be displayed to the user by the \emph{View}.

\section{Technologies}

\subsection*{Framework technologies}

\paragraph*{Kotlin}

Kotlin\footnote{https://kotlinlang.org/} is a cross-platform, strong statically typed, general-purpose high-level programming language with type inference.
It took inspiration from several programming languages including Java, Scala and others.
Born as an object-oriented programming language, it includes a lot of functional programming elements such as first-class support for higher-order functions and lambda literals.
The rise of Kotlin is testified by the fact that Google chose it as the official language for Android development, replacing Java.

In the beginning, Kotlin was developed as a JVM language, and support for multi-platform development was added recently. 
It has great advantages as it reduces time spent writing and maintaining the same code for different platforms while retaining the flexibility and benefits of native programming.

\paragraph*{YAML}
To provide the user with the ability to write a benchmark configuration file, various options were considered, such as using JSON, YAML, Google Protobuf or even developing a DSL. 
The choice fell on YAML\footnote{https://yaml.org/}, a human-readable data serialization language. It is a superset of JSON, which means that any valid JSON file is also a valid YAML file.
YAML is commonly used in applications where data needs to be represented in a format that is easy for both humans to read and write, as well as for machines to parse and generate.
It is used as a configuration language in different projects, such as Kubernetes, Docker, GitHub Actions, and many more.

\subsection*{DevOps technologies}
DevOps engineering is a software development methodology that aims at communication, collaboration and integration among all workers around an IT project. 
This set of techniques responds to interdependencies between software development and relative IT operations, allowing a faster and more efficient organization of software products and services.
The following paragraphs describe which DevOps techniques have been used in the making of the system, focusing on the advantages that each procedure has brought.

\paragraph*{Repository management}
The work is hosted on GitHub\footnote{https://www.github.com/} and uses Git\footnote{https://www.git-scm.com/} as a DVCS (Distributed Version Control System).
A DVCS is a version control system that allows multiple users to work on the same codebase at the same time and keeps track of every change made.
The project was developed following a customized version of Git Flow. 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/gitflow.png}
  \caption{Git Flow.}
  \label{fig:git-flow}
\end{figure}

The repository consists of a master branch and several feature branches.
The master branch is the development reference: all feature branches originate from it, and at the end of their existence, they are merged into the master branch.
The feature branches are related to the developed features. 
For each feature to be implemented, a feature branch was created, and at the end of the development, a pull request was made to merge the content into the master branch.
All feature branches followed this naming convention: \textit{feature/{feature-name}}

\paragraph*{Build automation}
Build Automation refers to the automation of the build lifecycle of a project, the process from source code to product release and distribution. 
This allows automating operations that were previously done manually, making software deployment more efficient and less error-prone.
In this work, Gradle\footnote{https://gradle.org/}, one of the most famous and widely used build systems, is used. 
Gradle is primarily used to manage dependencies with external libraries used in the project (e.g., the library for parsing YAML files).
With Gradle, it is possible to define custom tasks, which are essentially atomic operations on the project that have input and output files and can depend on other tasks. 
This functionality has been leveraged to define the task that creates the framework's JAR file, which is then uploaded to GitHub during the release phase.

\begin{lstlisting}[style=my-kotlin, language=my-kotlin, caption={Custom task to generate the JAR file}]
  tasks.withType<ShadowJar> {
    archiveFileName.set("testbed.jar")
    manifest {
        attributes(
            mapOf(
                "Implementation-Title" to "Testbed",
                "Implementation-Version" to rootProject.version.toString(),
                "Main-Class" to "testbed.Testbed",
            ),
        )
    }
}
\end{lstlisting}

\paragraph*{Continuous Integration}
Continuous Integration is a software development practice in which developers regularly merge their code changes into a central repository, after which automated builds and tests are run.
The key goals of continuous integration are to find and address bugs quicker, improve software quality, and reduce the time it takes to validate and release new software updates.
In this work, GitHub Actions is used to automate the process of building, testing, and deploying the framework.
The workflow of a GitHub Action is defined in a YAML file, which is stored in the repository under the path \textit{.github/workflows}.

\Cref{lst:ci-build} shows the build job of the CI/CD workflow, which runs the tests on different operating systems.

\begin{lstlisting}[language=yaml, label={lst:ci-build}, caption={CI/CD workflow: build job.}]
name: CI/CD Process

on:
  workflow_call:
  workflow_dispatch:

jobs:
  build:
    strategy:
      matrix:
        os: [ windows-2022, macos-12, ubuntu-22.04 ]
    runs-on: ${{ matrix.os }}
    concurrency:
      group: build-${{ github.workflow }}-${{ matrix.os }}-${{ github.event.number || github.ref }}
      cancel-in-progress: true
    steps:
      - name: Checkout
        uses: DanySK/action-checkout@0.2.14
      - name: Test
        run: ./gradlew test
\end{lstlisting}

\Cref{lst:ci-build} depicts the release job of the CI/CD workflow.

\begin{lstlisting}[language=yaml, label={lst:ci-release}, caption={CI/CD workflow: release job.}]
  jobs:
    release:
      concurrency:
        # Only one release job at a time. Strictly sequential.
        group: release-${{ github.workflow }}-${{ github.event.number || github.ref }}
      needs:
        - build
      runs-on: ubuntu-latest
      if: >-
        !github.event.repository.fork
        && (
          github.event_name != 'pull_request'
          || github.event.pull_request.head.repo.full_name == github.repository
        )
      steps:
        - name: Checkout
          uses: actions/checkout@v4.1.1
          with:
            token: ${{ secrets.GH_TOKEN }}
        - name: Find the version of Node from package.json
          id: node-version
          run: echo "version=$(jq -r .engines.node package.json)" >> $GITHUB_OUTPUT
        - name: Install Node
          uses: actions/setup-node@v4.0.2
          with:
            node-version: ${{ steps.node-version.outputs.version }}
        - name: Release
          env:
            GH_TOKEN: ${{ secrets.GH_TOKEN }}
          run: |
            npm install
            npx semantic-release
  \end{lstlisting}

\paragraph*{Versioning and Releasing}
For commits, Conventional Commits\footnote{https://www.conventionalcommits.org/en/v1.0.0/} are used, a convention that provides a set of possible commits, 
each with a different semantic, allowing the definition of the software version number based on the commit history.

The following set of commits is used:
\begin{itemize}
  \item \textbf{Major Release} 
  \begin{itemize}
    \item Any commit terminating \textit{!} causes a \emph{breaking change}
  \end{itemize}

  \item \textbf{Minor Release} 
  \begin{itemize}
    \item Commit type \emph{feat} with any scope.
  \end{itemize}

  \item \textbf{Patch Release} 
  \begin{itemize}
    \item Commit type \emph{fix} with any scope.
    \item Commit type \emph{docs} with any scope.
    \item Commit type \emph{chore} with scope \textit{core-deps}.
  \end{itemize}

  \item \textbf{No Release} 
  \begin{itemize}
    \item Commit type \emph{test} with any scope.
    \item Commit type \emph{ci} with any scope.
    \item Commit type \emph{chore} with scope \textit{deps}.
    \item Commit type \emph{refactor} with scope \textit{deps}.
  \end{itemize}
\end{itemize}

Thanks to the use of semantic release, it was possible to automate the versioning and releasing of the software. 
Every time a push is made to the master branch, semantic release calculates the version number and creates a release on GitHub, uploading the necessary assets.

The version number is defined in the format \emph{X.Y.Z} where X is a major version, Y is the minor version and Z is the patch version.

%----------------------------------------------------------------------------------------
\chapter{Validation}
%----------------------------------------------------------------------------------------

\section{Testing}

\subsection*{Unit Testing}
Unit testing is a software testing method to test individual units or components of software.
In this work, we use it to test the behavior of the various components of the framework.

Given the nature of the work, most components require human judgment to be tested. 
This is the case for the \emph{Executor} and \emph{Listener} components, which are responsible for starting the simulator and reading its output, respectively.
The tests on this component were conducted by repeatedly running the framework with different scenarios and checking the output manually.
The \emph{Parser} component, on the other hand, was tested automatically, to check if the parser correctly translates the input file into the benchmark model.

\begin{lstlisting}[style=my-kotlin, language=my-kotlin, caption={Parser tests}]
  class ParserTest: FreeSpec({
    "The parser" - {
        val parser: Parser = ParserImpl()
        "should parse a partial input file" {
            // ARRANGE
            val expectedBenchmark = simpleBenchmarkBuilder()
            // ACT
            val benchmark = parser.parse("src/test/resources/SimpleBenchmark.yml")
            // ASSERT
            assert(benchmark == expectedBenchmark)
        }
        "should parse a full input file" {
            val expectedBenchmark = fullBenchmarkBuilder()
            val benchmark = parser.parse("src/test/resources/FullBenchmark.yml")
            assert(benchmark == expectedBenchmark)
        }
        "should fail to parse a wrong file" {
            assertThrows<InvalidPropertyValueException> {
                parser.parse("src/test/resources/WrongBenchmark.yml")
            }
        }
    }
})
\end{lstlisting}

\subsection*{Integration Testing}

Integration testing is a software testing method to test the behavior of the various components of the software when integrated.
In this work, we use it to test the behavior of the simulators supported by the framework.

Checking if a simulator is running a scenario in the right way is not an easy task to do automatically, as it requires human judgment.
Therefore the integration tests are performed manually.

\paragraph*{NetLogo}
NetLogo was tested by running one of the bundled models, the Wolf Sheep Predation model.

The first time, the simulation was launched through the framework in headless mode, without a graphical interface.
Several logs were added to monitor the benchmark execution, and everything went as expected:
the parser generated the benchmark model, which was used by the controller and executor to launch the simulation.
At the end of the simulation, an output file containing the expected information was generated.
This CSV file was then transformed into a data structure within the framework containing the results.

The second execution was launched again through the framework, but this time with the graphical interface activated. 
The same steps as before were observed, with the addition of the simulation being visually displayed.

Comparing the two executions and conducting further ones, no differences were noticed. 
We can therefore say that NetLogo has been integrated correctly into the framework.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/netlogo-sim.png}
  \caption{NetLogo simulation launched by the framework}
\end{figure}

\paragraph*{Alchemist}
The same process was applied to test the integration with the Alchemist simulator.

To test the behavior of Alchemist, a program that computes the gradient, a typical problem known in the literature, was chosen. 
The first execution was done with the graphical interface activated, which allowed verifying the correct execution of the simulation, thanks to the graphical effects applied to the nodes.
The second execution was done headless and provided the same results as the first. This process was repeated for both incarnations supported by the framework, namely Protelis and Sapere.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/gradient-execution.png}
  \caption{Alchemist simulation launched by the framework}
\end{figure}

\section{Evaluation}

In this section, we will provide an example of how the framework can be used to benchmark \ac*{CAS}.

\paragraph*{Definition and execution of a benchmark}

We start by defining a problem, which is the computation of the gradient in a grid of nodes.
This is a known scenario in the literature, it consists of computing the distance of each node in the grid from a source node.
The solution to be tested is an implementation of the gradient computation in the Sapere incarnation of Alchemist.

We create a benchmark configuration file, which contains all the information about the benchmark to be executed.

\begin{lstlisting}[language=yaml, caption={Case of study: benchmark configuration file}]
strategy:
  executionOrder:
    - Alchemist-SAPERE-gradient

simulators:
  - name: ALCHEMIST
    simulatorPath: "./"
    scenarios:
      - name: Alchemist-SAPERE-gradient
        description: A benchmark to test the gradient computation in the Sapere incarnation
        input: "sapere-gradient.yml"
        repetitions: 1
\end{lstlisting}

After the benchmark configuration file, we define the input file for the Alchemist simulation:

\begin{lstlisting}[caption={Case of study: Alchemist input file}]
incarnation: sapere
launcher:
  type: HeadlessSimulationLauncher
seeds:
  scenario: 1
  simulation: 1
network-model:
  type: ConnectWithinDistance
  parameters: [0.35]

  _send: &id001
  - {time-distribution: 0.1, program: '{source} --> {source} {gradient, 0}'}
  - {time-distribution: 1, program: '{gradient, N} --> {gradient, N} *{gradient, N+#D}'}
  - program: >
      {gradient, N}{gradient, def: N2>=N} --> {gradient, N}
  - time-distribution: 0.1
    program: >
      {gradient, N} --> {gradient, N + 1}
  - program: >
      {gradient, def: N > 30} -->

deployments:
  type: Grid
  parameters: [-5, -5, 5, 5, 0.25, 0.25, 0.1, 0.1]
  contents:
    in:
      type: Rectangle
      parameters: [-0.5, -0.5, 1, 1]
    molecule: source
  programs: *id001
  
export:
  - type: CSVExporter
    parameters: {fileNameRoot: export, interval: 5, exportPath: ./}
    data: 
      - time
      - molecule: gradient, value
      property: value
      aggregators: [mean]
      value-filter: onlyfinite
terminate:
  - type: afterTime
    parameters: 100
\end{lstlisting}

Now we define the processing function which will be in charge of aggregating the data and returning the result.
The function is implemented without the use of any external library, using pure Kotlin.
It is not recommended to do so, as Kotlin is not a data analysis language and the code itself is not very readable.

\lstset{style=my-kotlin}
\begin{lstlisting}[language=my-kotlin, caption={Case of study: Output processing function.}]
  val timeValues = benchmarkOutput["Alchemist-SAPERE-gradient-1"]!!["time"]
  val gradientValues = benchmarkOutput["Alchemist-SAPERE-gradient-1"]!!["value[mean]"]
  val timeToStabilize = gradientValues.reversed().foldRight(Triple(0.00, 0, 0)) { value, acc ->
      if (acc.second == 0) {
          if (value == acc.first && value != 0.00) Triple(
              acc.first,
              gradientValuesDouble.indexOf(acc.first),
              acc.third - 1
          ) else Triple(value, 0, acc.third + 1)
      } else {
          acc
      }
  }
  return listOf(
      ScenarioResult(
          "Time to stabilize: ",
          listOf(timeValuesDouble[timeToStabilize.third]),
          VisualisationType.SINGLE_VALUE
      )
  )
\end{lstlisting}

The function looks at the gradient value and returns the time at which the gradient stabilizes.
A stable gradient is defined as two consecutive values that are the same and different from zero.
To make the comparison values of type \textit{Double} are used, with a precision of two decimal places.

We can observe the result of the benchmark execution, along with the output data of the simulator.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/result-A.png}
  \caption{Case of study: Benchmark result A}
\end{figure}

\paragraph*{Compare a new solution to the reference}

We want to create a new solution for the gradient computation problem and compare it to the reference solution.
To do so, we can use the benchmark configuration file and the processing function used for the reference solution,
develop our own solution and run the benchmark to get the result.

To define the solution we take the Alchemist input file and change the time distribution of the reactions.

\begin{lstlisting}[language=yaml, caption={Case of study: benchmark configuration file}]
  _send: &id001
  - {time-distribution: 1, program: '{source} --> {source} {gradient, 0}'}
  - {time-distribution: 2, program: '{gradient, N} --> {gradient, N} *{gradient, N+#D}'}
  - program: >
      {gradient, N}{gradient, def: N2>=N} --> {gradient, N}
  - time-distribution: 1
    program: >
      {gradient, N} --> {gradient, N + 1}
  - program: >
      {gradient, def: N > 30} -->
\end{lstlisting}

We can observe the result of the benchmark execution, and compare them against the reference solution.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/result-B.png}
  \caption{Case of study: Benchmark result B}
\end{figure}

The objective of this case study was not to assess particular aspects or the performance of a CAS. 
Instead, a basic metric was employed to estimate the system's stabilization speed.
This experiment illustrated the feasibility of defining a benchmark, executing it, and deriving a meaningful outcome for the user. 
It serves as a reference for testing and evaluating new solutions without the need to redefine the benchmark; modifications can be made solely by altering the solution. 
As a result, the framework requirements outlined in the analysis phase are deemed fulfilled.

%----------------------------------------------------------------------------------------
\chapter{Conclusion and Future Work}
%----------------------------------------------------------------------------------------

The work presented in this thesis has resulted in the development of a testbed prototype designed for benchmarking \ac*{CAS}.
The objective of the framework was to provide a tool that integrates the most widely used simulators in the field of \ac*{CAS}.
This allows users to assess a solution's behavior in different scenarios, utilizing different simulators, and facilitating the comparison of the obtained results.

To achieve this goal, the framework was designed to be flexible and extensible.
Users can express benchmark configurations in a straightforward and intuitive way, and integrate new simulators without breaking the existing structure.

A pivotal aspect of the framework lies in its abstract design. 
Each system component has a general behavior that is independent of the simulator, yet remains incomplete.
This architectural choice enables users to extend the framework by incorporating specific logic for new simulators.
For the users who are not interested in introducing support for new simulators, the testbed still serves as a useful tool for evaluating solution behavior in specific scenarios.
Furthermore, the framework promotes community collaboration, enabling individuals to define benchmarks and share results.
This approach allows users to incorporate their solutions into pre-configured benchmarks, supporting result comparisons.

In our opinion, the testbed constitutes a promising foundation for the creation of a comprehensive tool for testing \ac*{CAS}.
It addresses the key challenges in the development of an open benchmarking platform, providing a solid base for future works.

\paragraph*{Future Works}

Despite the work that has been done, the testbed is still in its early stages and some features are missing for it to become a complete tool for testing \ac*{CAS}.
The scientific community requires a high standard of quality and reliability, and the framework must be able to provide it.
This section will provide a list of possible future works that could be done.

\begin{itemize}
  \item \textbf{Increase the number of supported simulators} 
  The framework currently supports two simulators, Alchemist and NetLogo.
  In the field of \ac*{CAS}, there are other simulators used to test different aspects of the systems.
  Supporting a wide range of simulators is necessary for the framework to be useful and relevant within the scientific community.
  Moreover, this would enable the user to test the behavior of the system in its entirety. 
  \item \textbf{Multi-platform support}
  At the time of writing, the testbed has been developed for JVM platforms. 
  Switching to Kotlin-Multiplatform would allow the framework to be compiled for different target platforms, such as JavaScript, iOS, and native.
  The ability to provide support for different platforms will help the framework to be more widely adopted.
  \item \textbf{Graphical User Interface}
  Currently, it is possible to interact with the framework only through a \ac*{CLI}.
  The user experience is known to be one of the most important aspects of software, as it can make the difference between a successful and an unsuccessful product.
  It is fundamental to provide the user with a graphical interface to interact with the application, both for the benchmark configuration and the visualization of the results.
  Benchmarking often involves comparing data from different solutions and a graphical interface would make this process easier and more intuitive.
  \item \textbf{Provide a way to download simulators}
  At present, the framework requires the user to download the simulators manually.
  Providing the user with a way to download a simulator, using a simple command, would make the framework more user-friendly.
  Moreover, this would ease the user from the task of manually downloading the simulators.
  \item \textbf{Maintaining benchmarks history}
  The comparison of new and existing solutions to well-known problems is a fundamental aspect when developing innovative solutions.
  Keeping a history of benchmark results in an online repository would facilitate such comparisons. 
  Currently, users lack access to a database containing known benchmarks, various solutions, and their respective results. 
  Providing them with this database would streamline the comparison process.
\end{itemize}

%----------------------------------------------------------------------------------------
% BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\backmatter

%\nocite{*} % comment this to only show the referenced entries from the .bib file

\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}
